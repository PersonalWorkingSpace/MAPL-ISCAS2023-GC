<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Learned Hierarchical B-frame Coding with Adaptive Feature Modulation for YUV 4:2:0 Content</title>
  <script type="text/javascript" src="assets/latexit.js"></script>
  <script type="text/javascript">
    LatexIT.add('p', true);
  </script>

  <!-- CSS includes -->
  <link href="assets/bootstrap.css" rel="stylesheet">
  <link href="assets/css.css" rel="stylesheet" type="text/css">
  <link href="assets/mystyle.css" rel="stylesheet">
  <link href="assets/lightbox2-2.11.3/dist/css/lightbox.css" rel="stylesheet" />

</head>

<body>

  <div id="header" class="container-fluid">
    <div class="row">
      <h1>Learned Hierarchical B-frame Coding with <br /> Adaptive Feature Modulation for YUV 4:2:0 Content</h1>
      <div class="authors">
        Mu-Jung Chen, Hong-Sheng Xie, Cheng Chien, Wen-Hsiao Peng, Hsueh-Ming Hang
      </div>
      <div class="conference">
        ISCAS 2023
      </div>
    </div>

    <p style="text-align:center;">
      <a href="https://en.nycu.edu.tw/" target="_blank"><img src="assets/210204-NYCU (1).png" height="80"></a>
       
      <a href="http://mapl.nctu.edu.tw/" target="_blank"><img src="assets/mapl_logo.png" height="100"></a>
    </p>
  </div>
  <div class="container" id="abstractdiv">
    <h2>Abstract</h2>
    This paper introduces a learned hierarchical Bframe coding scheme in response to the Grand Challenge
    on Neural Network-based Video Coding at ISCAS 2023. We address specifically three issues, including (1) B-frame coding,
    (2) YUV 4:2:0 coding, and (3) content-adaptive variable-rate coding with only one single model. Most learned video codecs
    operate internally in the RGB domain for P-frame coding. Bframe coding for YUV 4:2:0 content is largely under-explored.
    In addition, while there have been prior works on variable-rate coding with conditional convolution, most of them fail to consider
    the content information. We build our scheme on conditional augmented normalized flows (CANF). It features conditional
    motion and inter-frame codecs for efficient B-frame coding. To cope with YUV 4:2:0 content, two conditional inter-frame codecs
    are used to process the Y and UV components separately, with the coding of the UV components conditioned additionally on the Y
    component. Moreover, we introduce adaptive feature modulation in every convolutional layer, taking into account both the content
    information and the coding levels of B-frames to achieve contentadaptive variable-rate coding. Experimental results show that our
    model outperforms x265 and the winner of last year’s challenge on commonly used datasets in terms of PSNR-YUV.
  </div>

  <div class="container" id="banner">
    <h2>Overview</h2>
    <p style="text-align:center;">
      <a href="assets/architecture.png" data-lightbox="arch"><img src="assets/architecture.png" data-lightbox="arch"
          width="65%"></a>
    </p>
    <br>
    <p>
      The figure presents an overview of our proposed method. As shown, the encoding of a B-frame $x^{420}_t$ begins with 
      using the motion estimation network (MENet) operating internally in the YUV 4:4:4 domain to obtain 
      bi-directional optical flow maps $m_{t\to t-k}, m_{t\to t+k}$ according to its two reference frames $\hat{x}^{420}_{t-k}, \hat{x}^{420}_{t+k}$, respectively. 
      The resulting flow maps are compressed jointly by the CANF-based conditional motion codec ($M, M^{-1}$) 
      given the conditioning signals $m^p_{t\to t-k}, m^p_{t\to t+k}$ generated by the motion prediction network (MPNet). 
      The decoded flow maps $\hat{m}_{t\to t-k}, \hat{m}_{t\to t+k}$ are used for bi-directional motion compensation. 
      Particularly, we adopt two separate motion compensation networks (MCNet-<em>Y</em>, MCNet-<em>UV</em>) to synthesize 
      the motion-compensated frames $\hat{x}^y_c, \hat{x}^{uv}_c$ for Y and UV components, respectively. $\hat{x}^y_c, \hat{x}^{uv}_c$ 
      serve as the conditioning signals for conditional inter-frame coding of $x^y_t, x^{uv}_t$ to obtain the reconstructed Y and UV components
       $\hat{x}^y_t, \hat{x}^{uv}_t$, respectively. Notably, for coding the UV components, we introduce the reconstructed Y component as 
       an additional conditioning signal. The following sections elaborate on these proposed modules.
    </p>
    </div>      
      <div class="container" id="method">
        <h2>Method</h2>
        <p style="text-align:center;">
          <a href="assets/CVFA_module.png" data-lightbox="cvfa"><img src="assets/CVFA_module.png" width="50%"></a>
        </p>
        <br>
        <p>
          Adaptive feature (AF) modulation is to adapt the feature distribution in every convolutional layer, 
          in order to achieve variable-rate compression with a single model and content-adaptive coding.
          The AF modulation is placed after every convolutional layer in the motion and inter-frame codecs. 
          As shown in the figure, it outputs channel-wise affine parameters, which are used to dynamically adjust the output feature distributions. 
          As compared to the previous works, our scheme has two distinctive features. 
          One is that we introduce the coding level $C$ of a B-frame as its contextual information to achieve hierarchical rate control. 
          This is motivated by the fact that with hierarchical B-frame coding, the reference quality of a B-frame varies with its coding level.
          The additional contextual information from the coding level allows greater flexibility in adjusting the bit allocation among B-frames.
          Additionally, our AF module incorporates a global average pooling (GAP) layer to summarize the input feature maps
          with a 1-D feature vector. As such, our AF module is able to adapt the feature distribution in a content-adaptive manner.
        </p>
      </div>
      
      <div class="container" id="paperdiv">
        <h2>Paper</h2>
        <a href="assets/paper.pdf"
          download="Learned Hierarchical B-frame Coding with Adaptive Feature Modulation for YUV 4:2:0 Content.pdf">
          <div class="thumbs">
        <img src="assets/thumbnails/0001.jpg" width="19%">
        <img src="assets/thumbnails/0002.jpg" width="19%">
        <img src="assets/thumbnails/0003.jpg" width="19%">
        <img src="assets/thumbnails/0004.jpg" width="19%">
        <img src="assets/thumbnails/0005.jpg" width="19%">
      </div>
    </a>

    <div class="container" id="exp_results">
      <h2>Results</h2>
      <p>
        The figure shows the rate-distortion comparison and subjective quality comparison. 
        First, our method outperforms x265 and <a href="https://arxiv.org/abs/2210.08225">the learned codec</a> 
        by a large margin across all the datasets. This is attributed to the use of more efficient B-frame coding. 
        Second, the proposed method is seen to be inferior to HM under the random access configuration, which represents a
        much stronger baseline method for B-frame coding.
        However, our proposed method shows better visual quality with less color bias and blocking artifacts for some content.
        <b>Click on image to enlarge it.</b>
      </p>
        <p style="text-align:center;">
          <a href="assets/figures/RD.png" data-lightbox="RD"><img src="assets/figures/RD.png" data-lightbox="RD"
              width="100%"></a>
        </p>
        <div class="sep">
          <hr>
        </div>
        <p>
          <a href="assets/figures/diff.png" data-lightbox="diff"><img src="assets/figures/diff.png" data-lightbox="diff"
            width="100%"></a>
        </p>
    </div>

    <div id=footer><br></div>
    <!-- Javascript includes -->
    <script src="assets/jquery-1.js"></script>
    <script src="assets/bootstrap.js"></script>
    <script src="assets/lightbox2-2.11.3/dist/js/lightbox.js"></script>


</body>

</html>